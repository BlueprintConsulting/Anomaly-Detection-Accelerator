{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cfb028cd-32c7-43fa-8ef9-ccede845e139",
     "showTitle": true,
     "title": "Network Threat Detection System (NTDS)"
    }
   },
   "source": [
    "This series of notebooks is intended to develop and/or explore various elements of work needed to create a Network Threat Detection System (\"NTDS\").  The approach being used is traffic capture, anomaly detection, development of threat models, online refinement of models, development of an alert manager UI, and alert broadcasting.  We expect to implement a product architecture that looks something like \n",
    "this:\n",
    "\n",
    "![ntds_arch.png](/files/ntds_arch.png)\n",
    "\n",
    "The development of the elements will be distributed across several notebooks like this:</p>\n",
    "\n",
    "* Primus -- Explores and implements Network Traffic Anomaly Detection. \n",
    "* Secundus -- Implements a streaming solution for aggregating data from chosen network switches/sources using Kafka and SparkStreaming\n",
    "* Tertius -- Uses pyspark to implement a threat identification, threat-model, model refinement loop for creating and improving threat models over time.\n",
    "* Quartus -- Develops and refines the Alert Manager GUI \n",
    "* Quintus -- Develops and refines the Alert Broadcast module\n",
    "\n",
    "Implementing all of the pieces of this project will take some time. FOr the purpose of the X-Challenge project, only the \"Primus\" notebook will be completed.\n",
    "\n",
    "<i>Note: There is \"starter notebook\" in the Data Bricks Samples Menagerie about \"Threat Detection using DNS Data\".  The approach there focuses on recognizing manglings of text DNS strings and checking those manglings against known exploits like phishing scams. The approach here is completely different and is focused on the detailed properties of the data flow rather than DNS name-mangling.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2e95d7d5-0e99-40a3-ab38-62ed478e5193",
     "showTitle": true,
     "title": "Primus"
    }
   },
   "source": [
    "This notebook uses a static data set previously captured from the Universidad Del Caucau in Popayan Columbia. This dataset was captured from network switches at the university using an open source tool (https://github.com/jsrojas/FlowLabeler) in April and May of 2017.  The dataset was freely posted on the \"Kaggle\" data science website (see https://www.kaggle.com/datasets/jsrojas/ip-network-traffic-flows-labeled-with-87-apps) and contains roughly 3.6 million rows of data.  Each row has 87 statistical attributes (\"micro-features\") characterizing the traffic and is labeled by application family (\"Google\", \"Facebook\", \"Apple\", \"Windows Update\", \"Skype\", and so forth).  For convenience the data has been preloaded into a pandas dataframe and saved dill-contained pandas dataframe.  <i>That</i> file has been uploaded to DataBricks as <i>Dataset-Unicauca-Version2-87Atts.dill.gz</i> and will be used in lieu of a streaming data source for the purpose of developing the ideas for Anomaly Detection in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "908d19ab-a06d-40c7-abde-8d273c274bd9",
     "showTitle": true,
     "title": "Imports"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/continuum/miniconda3/envs/ncemc/lib/python3.11/site-packages (24.3.1)\n",
      "Requirement already satisfied: tqdm in /opt/continuum/miniconda3/envs/ncemc/lib/python3.11/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time, math, gzip\n",
    "import pandas as pd                                                                                 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from math import pi\n",
    "import pyspark\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "#========================================\n",
    "!python -m pip install --upgrade pip\n",
    "#========================================\n",
    "try:\n",
    "    import json\n",
    "except:\n",
    "    !pip install --upgrade json\n",
    "    import json\n",
    "#========================================\n",
    "try:\n",
    "    import dill\n",
    "except:\n",
    "    !pip install --upgrade dill\n",
    "    import dill\n",
    "#========================================\n",
    "try:\n",
    "    import sklearn\n",
    "except:\n",
    "    !pip install --upgrade sklearn\n",
    "finally:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "#========================================\n",
    "try:\n",
    "    import fastparquet, pyarrow\n",
    "except:\n",
    "    !pip install --upgrade fastparquet\n",
    "    !pip install --upgrade pyarrow\n",
    "    import fastparquet, pyarrow\n",
    "#========================================\n",
    "try:\n",
    "    import requests\n",
    "except:\n",
    "    !pip install --upgrade requests\n",
    "    import requests\n",
    "#========================================\n",
    "try:\n",
    "    import urllib.request\n",
    "except:\n",
    "    !pip install --upgrade urllib3\n",
    "    import urllib.request\n",
    "#========================================\n",
    "try:\n",
    "    import gdown\n",
    "except:\n",
    "    !pip install --upgrade gdown\n",
    "    import gdown\n",
    "#========================================\n",
    "try:\n",
    "    from tqdm import faux_tqdm\n",
    "except:\n",
    "    !pip install --upgrade tqdm\n",
    "    from tqdm import tqdm    \n",
    "#========================================\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "except:\n",
    "    !pip install --upgrade geopandas\n",
    "    import geopandas as gpd\n",
    "#==========================================\n",
    "try:\n",
    "    from scipy.stats import mode\n",
    "except:\n",
    "    !pip install --upgrade scipy\n",
    "    from scipy.stats import mode\n",
    "#===========================================\n",
    "from bptools.utils.storage import places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "54212218-c31d-4ab1-8bb6-a2c6d6a56ed1",
     "showTitle": true,
     "title": "Functions"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Lots of functions...\n",
    "def public_address(ipaddr) -> bool:\n",
    "    \"\"\"\n",
    "    Small function for identifying whether an IP address is 'public'\n",
    "    (as opposed to 'private')\n",
    "\n",
    "    \"\"\"\n",
    "    private_starts = [\n",
    "        \"10.\", \"192.168\", \"172.16.\", \"172.17.\", \"172.18.\", \"172.19.\",\n",
    "        \"172.20.\", \"172.21.\", \"172.22.\", \"172.23.\", \"172.24.\", \"172.25.\",\n",
    "        \"172.26.\", \"172.27.\", \"172.28.\", \"172.29.\", \"172.30.\", \"172.31.\",\n",
    "    ]\n",
    "    for test in private_starts:\n",
    "        if (ipaddr.startswith(test)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def loader(data_name:str = \"Dataset-Unicauca-Version2-87Atts.parquet.gzip\", \n",
    "           verbose:bool = False,\n",
    "          ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function which fetches the traffic data...\n",
    "\n",
    "    \"\"\"\n",
    "    if (verbose):\n",
    "        bt = time.time()\n",
    "    data_path = os.path.join(places(\"datasets\"), data_name)\n",
    "    if (not os.path.exists(data_path)):\n",
    "        raise RuntimeError(f\"could not find {data_path}\")\n",
    "    else:\n",
    "        if (verbose):\n",
    "            print(f\"found {data_path}, loading...\")\n",
    "        df = pd.read_parquet(data_path)\n",
    "    #\n",
    "    # okay so we have the data, let's read it..\n",
    "    #\n",
    "    if (verbose):\n",
    "        et = time.time()\n",
    "        print(f\"read dataframe with shape {df.shape} in {round(et - bt, 3)} secs\")\n",
    "    return df\n",
    "\n",
    "def find_location(ipaddr:str) -> {}:\n",
    "    \"\"\"\n",
    "    given an IP address, search for information about the location\n",
    "    associated with it.\n",
    "\n",
    "    @TODO please improve this documentation\n",
    "\n",
    "    \"\"\"\n",
    "    #\n",
    "    # super trivial case: ip-api.com\n",
    "    #\n",
    "    search = f\"http://ip-api.com/json/{ipaddr}\"\n",
    "    request = urllib.request.Request(search)\n",
    "    response = urllib.request.urlopen(request).read()\n",
    "    location = json.loads(response.decode(\"utf-8\"))\n",
    "    lkeys = list(location.keys())\n",
    "    #\n",
    "    # the information in from ip-api.com *can* be quite good\n",
    "    # but if the ORG is the same as the ISP then it's probably\n",
    "    # only giving us the ISP's physical location information...\n",
    "    #\n",
    "    location[\"trust\"] = False\n",
    "    if (\"isp\" in lkeys) and (\"org\" in lkeys):\n",
    "        if (location[\"isp\"] != location[\"org\"]):\n",
    "            location[\"trust\"] = True\n",
    "    return location\n",
    "\n",
    "def find_country(ipaddr:str) -> str:\n",
    "    # country determinations are > 99.8% reliable\n",
    "    location = find_location(ipaddr)\n",
    "    return location[\"countryCode\"]\n",
    "\n",
    "def find_region(ipaddr:str) -> str:\n",
    "    location = find_location(ipaddr)\n",
    "    return location[\"region\"]\n",
    "\n",
    "def find_city(ipaddr:str) -> str:\n",
    "    location = find_location(ipaddr)\n",
    "    return location[\"city\"]\n",
    "\n",
    "def find_latlon(ipaddr:str) -> (float, float):\n",
    "    location = find_location(ipaddr)\n",
    "    return (location[\"lat\"], location[\"lon\"])\n",
    "\n",
    "class CacheError(Exception):\n",
    "    pass\n",
    "\n",
    "class Cache:\n",
    "    \"\"\"\n",
    "    A class for maintaining a caching memory of IP addresses and\n",
    "    a modest amount of metadata.\n",
    "    \n",
    "    @TODO: will want to allow for locking so as to provide for \n",
    "           some protection during multithreading...\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 cache_name:str = \"cache.dill\",\n",
    "                 overwrite:bool = False,\n",
    "                 verbose:bool = False,\n",
    "                 debug:bool = False,\n",
    "                 threshold:int = 100,\n",
    "                 throttled:bool = True,\n",
    "                 interval:float = 3.0, \n",
    "                ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.overwrite = overwrite\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "        self.threshold = threshold\n",
    "        self.record_count = 0\n",
    "        self.throttled = throttled\n",
    "        self.interval = interval\n",
    "        self.data_path = os.path.join(places(\"datasets\"), cache_name)\n",
    "\n",
    "        if (os.path.exists(self.data_path)) and (not self.overwrite):\n",
    "            self.load()\n",
    "            \n",
    "        if (not os.path.exists(self.data_path)) or (self.overwrite):\n",
    "            if (os.path.exists(self.data_path)):\n",
    "                os.remove(self.data_path)\n",
    "            self.data = {}\n",
    "\n",
    "    def search(self, ipaddr:str) -> {}:\n",
    "        \"\"\"\n",
    "\n",
    "        @TODO: consider the how the keys() is used...\n",
    "\n",
    "        \"\"\"\n",
    "        if (not ipaddr in self.data.keys()):\n",
    "            if (self.verbose) or (self.debug):\n",
    "                print(f\"we haven't seen {ipaddr} before, looking for it...\")\n",
    "            #\n",
    "            # well we haven't seen this one before so look it up...\n",
    "            #\n",
    "            expected_wait = time.time() + self.interval\n",
    "            self.data[ipaddr] = find_location(ipaddr)\n",
    "            #\n",
    "            # we're using the FREE version of the API ('throttled' == True)\n",
    "            # in principle we're only allowed to make 45 reqs/min -- so no\n",
    "            # faster than 1.33s / query...\n",
    "            #\n",
    "            keep_waiting = time.time() - expected_wait\n",
    "            if ((keep_waiting < 0) and (self.throttled)):\n",
    "                time.sleep(-keep_waiting)\n",
    "            #\n",
    "            # ...initialize a reference counter...\n",
    "            #\n",
    "            self.data[ipaddr][\"references\"] = 0\n",
    "            self.record_count += 1\n",
    "        elif ((ipaddr in self.data.keys()) and (self.data[ipaddr][\"status\"] == \"success\")):\n",
    "            if (self.debug):\n",
    "                print(f\"we HAVE seen {ipaddr} before and read it successfully...\")\n",
    "            #\n",
    "            # just increment the reference counter if we already\n",
    "            # have a trusted valid read...\n",
    "            #\n",
    "            self.data[ipaddr][\"references\"] += 1\n",
    "        elif ((ipaddr in self.data.keys()) and (self.data[ipaddr][\"status\"] != \"success\")):\n",
    "            if (self.debug):\n",
    "                print(f\"we HAVE seen {ipaddr} before and but FAILED to read it successfully...\")\n",
    "            #\n",
    "            # we have seen it before but we have NOT read it successfully, so let's try\n",
    "            # to read it again...\n",
    "            #\n",
    "            test_data = find_location(ipaddr)\n",
    "            if (test_data[\"status\"] == \"success\"):\n",
    "                if (self.debug):\n",
    "                    print(f\"we now have valid data for {ipaddr}, updating\")\n",
    "                #\n",
    "                # when we get valid data overwrite what was there before...\n",
    "                #\n",
    "                for key in test_data.keys():\n",
    "                    self.data[ipaddr][key] = test_data[key]\n",
    "            self.data[ipaddr][\"references\"] += 1\n",
    "        #\n",
    "        # by now there definitely is a record in hand...\n",
    "        #\n",
    "        target = self.data[ipaddr]\n",
    "        #\n",
    "        # regardless, update the reference counter...\n",
    "        #\n",
    "        if (self.record_count == self.threshold):\n",
    "            print(f\"have read {self.record_count} new addresses, updating cache\")\n",
    "            #\n",
    "            # save cache to disk every 'self.threshold' records...\n",
    "            #\n",
    "            self.record_count = 0\n",
    "            self.dump()\n",
    "            self.load()\n",
    "        #\n",
    "        # so return it...\n",
    "        #\n",
    "        if (self.verbose) or (self.debug):\n",
    "            if (target['status'] == 'success'):\n",
    "                message = (f\"ipaddr {ipaddr} is from \" +\n",
    "                           f\"{target['city']}, \" +\n",
    "                           f\"{target['region']}, \" +\n",
    "                           f\"{target['countryCode']}\")\n",
    "                print(message)\n",
    "            elif (target['message'] == 'private range'):\n",
    "                print(\"this is a private (internal) address\")\n",
    "        return target\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        @TODO: please improve this documentation\n",
    "\n",
    "        \"\"\"\n",
    "        self.data = dill.load(open(self.data_path, \"rb\"))\n",
    "        return None\n",
    "\n",
    "    def dump(self):\n",
    "        \"\"\"\n",
    "        @TODO: please improve this documentation\n",
    "\n",
    "        \"\"\"\n",
    "        dill.dump(self.data, open(self.data_path, \"wb\"))\n",
    "        return None\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        @TODO: please improve this documentation\n",
    "\n",
    "        \"\"\"\n",
    "        nkeys = len(list(self.data.keys()))\n",
    "        strrep = (\"<Cache \" +\n",
    "                  f\"\\n\\tdata_path = {self.data_path},\" + \n",
    "                  f\"\\n\\toverwrite = {self.overwrite},\" + \n",
    "                  f\"\\n\\tverbose = {self.verbose},\" + \n",
    "                  f\"\\n\\tdebug = {self.debug},\" + \n",
    "                  f\"\\n\\tthreshold = {self.threshold},\" + \n",
    "                  f\"\\n\\tdata = <dict with {nkeys} unique addresses>\" +\n",
    "                  f\"\\n\\t\\>\")\n",
    "        return strrep\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        @TODO: please improve this documentation\n",
    "\n",
    "        \"\"\"\n",
    "        return __str__()\n",
    "\n",
    "def traffic_proto_ranking(traffic_proto):    \n",
    "    #\n",
    "    # define them...\n",
    "    #\n",
    "    utp = np.unique(traffic_proto)\n",
    "    #\n",
    "    # initialize a dictionary...\n",
    "    #\n",
    "    utp_freqs = {}\n",
    "    for key in utp:\n",
    "        if (key not in utp_freqs.keys()):\n",
    "            utp_freqs[key] = 0\n",
    "    #\n",
    "    # count them...\n",
    "    #\n",
    "    for tp in traffic_proto:\n",
    "        utp_freqs[tp] += 1\n",
    "    #\n",
    "    # rank them...\n",
    "    #\n",
    "    ranking = []\n",
    "    for key in utp_freqs.keys():\n",
    "        ranking.append((utp_freqs[key], key))\n",
    "    sranking = sorted(ranking, reverse = True)\n",
    "    total = np.sum([s for (s,p) in sranking])\n",
    "    nsranking = [(100*s/total,p) for (s,p) in sranking]\n",
    "    return nsranking\n",
    "\n",
    "def display_metrics(population:{}, min_count:int = 1, num_bins = 20, show_histograms:bool = False, show_table:bool = True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if (show_table):\n",
    "        print(\"ProtocolName        median(F)    median(D)     median(V)     median(A)     median(R)     median(P)\")\n",
    "        print(\"----------------- ------------ ------------  ------------  ------------  ------------  ------------\")\n",
    "    \n",
    "    for proto in population.keys():\n",
    "        if ((show_table) and (population[proto][\"metrics\"][\"flowrate\"][\"count\"] >= min_count)):\n",
    "            print(\"%17s %12d %12d %12d %12d %12d %12d\" % \n",
    "                  ((proto+\"                 \")[0:17],\n",
    "                   population[proto][\"metrics\"][\"flowrate\"][\"median\"],\n",
    "                   population[proto][\"metrics\"][\"duration\"][\"median\"],\n",
    "                   population[proto][\"metrics\"][\"volume\"][\"median\"],\n",
    "                   population[proto][\"metrics\"][\"avpktsz\"][\"median\"],\n",
    "                   population[proto][\"metrics\"][\"dnuprat\"][\"median\"],\n",
    "                   population[proto][\"metrics\"][\"pktlenvar\"][\"median\"],\n",
    "                  )\n",
    "            )\n",
    "        #\n",
    "        # we're going to make 1 figure for each quantity for which we have at least\n",
    "        # 1000 observations...\n",
    "        #\n",
    "        main_title = 24\n",
    "        pane_title = 18\n",
    "        pane_text = 16\n",
    "        if (population[proto][\"metrics\"][\"flowrate\"][\"count\"] >= min_count):\n",
    "            #\n",
    "            # create the overall image and add the title...\n",
    "            #\n",
    "            fig, ax = plt.subplots(3, 3, figsize=(30, 30))\n",
    "            plt.suptitle(f\"Key Histograms for {proto}\", fontsize = main_title)\n",
    "            #\n",
    "            # 1st col, 1st row...\n",
    "            #\n",
    "            ax[0][0].set_title(\"Flow Rate (bytes/sec)\", fontsize = pane_title)\n",
    "            ax[0][0].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[0][0].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            ax[0][0].set_yscale('log')\n",
    "            ax[0][0].hist(population[proto][\"metrics\"][\"flowrate\"][\"data\"], color = \"blue\",  bins = num_bins)\n",
    "            #\n",
    "            # 1st col, 2nd row...\n",
    "            #\n",
    "            ax[0][1].set_title(\"Duration (secs)\", fontsize = pane_title)\n",
    "            ax[0][1].set_yscale('log')\n",
    "            ax[0][1].hist(population[proto][\"metrics\"][\"duration\"][\"data\"], color = \"red\", bins = num_bins)\n",
    "            ax[0][1].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[0][1].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            #\n",
    "            # 1st col, 3rd row...\n",
    "            #\n",
    "            ax[0][2].set_title(\"Volume (bytes)\", fontsize = pane_title)\n",
    "            ax[0][2].set_yscale('log')\n",
    "            ax[0][2].hist(population[proto][\"metrics\"][\"volume\"][\"data\"], color = \"green\", bins = num_bins)\n",
    "            ax[0][2].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[0][2].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            #\n",
    "            # 2nd col, 1st row...\n",
    "            #\n",
    "            ax[1][0].set_title(\"Average Packet Size (bytes)\", fontsize = pane_title)\n",
    "            ax[1][0].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[1][0].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            ax[1][0].set_yscale('log')\n",
    "            ax[1][0].hist(population[proto][\"metrics\"][\"avpktsz\"][\"data\"], color = \"magenta\",  bins = num_bins)\n",
    "            #\n",
    "            # 2nd col, 2nd row...\n",
    "            #\n",
    "            ax[1][1].set_title(\"Down Up Ratio\", fontsize = pane_title)\n",
    "            ax[1][1].set_yscale('log')\n",
    "            ax[1][1].hist(population[proto][\"metrics\"][\"dnuprat\"][\"data\"], color = \"cyan\", bins = num_bins)\n",
    "            ax[1][1].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[1][1].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            #\n",
    "            # 2nd col, 3rd row...\n",
    "            #\n",
    "            ax[1][2].set_title(\"Packet Length Variance\", fontsize = pane_title)\n",
    "            ax[1][2].set_yscale('log')\n",
    "            ax[1][2].hist(population[proto][\"metrics\"][\"pktlenvar\"][\"data\"], color = \"brown\", bins = num_bins)\n",
    "            ax[1][2].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[1][2].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            #\n",
    "            # 3rd col, 1st row...\n",
    "            #\n",
    "            ax[2][0].set_title(\"Forward Packet Length Mean (bytes)\", fontsize = pane_title)\n",
    "            ax[2][0].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[2][0].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            ax[2][0].set_yscale('log')\n",
    "            ax[2][0].hist(population[proto][\"metrics\"][\"fwdpktlen\"][\"data\"], color = \"orange\",  bins = num_bins)\n",
    "            #\n",
    "            # 3rd col, 2nd row...\n",
    "            #\n",
    "            ax[2][1].set_title(\"Backward Packet Length Mean (bytes)\", fontsize = pane_title)\n",
    "            ax[2][1].set_yscale('log')\n",
    "            ax[2][1].hist(population[proto][\"metrics\"][\"bwdpktlen\"][\"data\"], color = \"lightgreen\", bins = num_bins)\n",
    "            ax[2][1].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[2][1].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            #\n",
    "            # 3rd col, 3rd row...\n",
    "            #\n",
    "            ax[2][2].set_title(\"Active Mean\", fontsize = pane_title)\n",
    "            ax[2][2].set_yscale('log')\n",
    "            ax[2][2].hist(population[proto][\"metrics\"][\"actmean\"][\"data\"], color = \"indigo\", bins = num_bins)\n",
    "            ax[2][2].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[2][2].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            #\n",
    "            # save/display or whatever...\n",
    "            #\n",
    "            plt.savefig(os.path.join(places(\"images\"), f\"{proto}_histogram.png\"))\n",
    "            if (show_histograms):\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "    return None\n",
    "\n",
    "def display_world(slatlon, dlatlon, traffic_color, dur, bps, lons, lats, show_world = False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    all_traffic_image = os.path.join(places(\"images\"), \"all_traffic.png\")\n",
    "    if (show_world == False) and (os.path.exists(all_traffic_image)):\n",
    "        print(\"...found existing {all_traffic_image} so skipping rest of display_world(...)\")\n",
    "        return None\n",
    "    \n",
    "    print(\"...creating graphic of world traffic\")\n",
    "    fig, ax = plt.subplots(2, 1, figsize = (24, 24))\n",
    "    worldmap = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "    worldmap.plot(color = \"lightgrey\", ax = ax[0])\n",
    "    #\n",
    "    # the inbound traffic map...\n",
    "    #\n",
    "    main_title = 24\n",
    "    pane_title = 18\n",
    "    pane_text = 16\n",
    "    plt.suptitle(\"Network Traffic\", fontsize = main_title)\n",
    "    ax[0].set_title(\"Inbound\", fontsize = pane_title)\n",
    "    ax[0].set_xlim((-180.0, 180.0))\n",
    "    ax[0].set_xlabel(\"Longitude\", fontsize = pane_text)\n",
    "    ax[0].set_ylim((-90.0, 90.0))\n",
    "    ax[0].set_ylabel(\"Latitude\", fontsize = pane_text)\n",
    "    ax[0].scatter(lons, lats, marker = \",\", s = 1, alpha = 0.10, color = \"black\")\n",
    "    #\n",
    "    # the traffic direction vectors...\n",
    "    #\n",
    "    print(\"...overlaying inbound traffic vectors\")\n",
    "    for i in tqdm(range(len(bps))):\n",
    "        if (traffic_color[i] == \"green\"):\n",
    "            ax[0].plot([slatlon[i][0], dlatlon[i][0]], [slatlon[i][1], dlatlon[i][1]],\n",
    "                       color = traffic_color[i], lw = dur[i], alpha = bps[i])\n",
    "    #\n",
    "    # the outbound traffic map...\n",
    "    #\n",
    "    worldmap = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "    worldmap.plot(color = \"lightgrey\", ax = ax[1])\n",
    "    #\n",
    "    # the various endpoints...\n",
    "    #\n",
    "    ax[1].set_title(\"Outbound\", size = pane_title)\n",
    "    ax[1].set_xlim((-180.0, 180.0))\n",
    "    ax[1].set_xlabel(\"Longitude\", size = pane_text)\n",
    "    ax[1].set_ylim((-90.0, 90.0))\n",
    "    ax[1].set_ylabel(\"Latitude\", size = pane_text)\n",
    "    ax[1].scatter(lons, lats, marker = \",\", s = 1, alpha = 0.10, color = \"black\")\n",
    "    #\n",
    "    # the traffic direction vectors...\n",
    "    #\n",
    "    print(\"...overlaying outbound traffic vectors\")\n",
    "    for i in tqdm(range(len(bps))):\n",
    "        if (traffic_color[i] == \"red\"):\n",
    "            ax[1].plot([slatlon[i][0], dlatlon[i][0]], [slatlon[i][1], dlatlon[i][1]],\n",
    "                       color = traffic_color[i], lw = dur[i], alpha = bps[i])\n",
    "    print(\"...saving completed figure\")\n",
    "    plt.savefig(all_traffic_image)\n",
    "    if (show_world):\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    return None\n",
    "\n",
    "def populate_cache(world_graphics:bool = True, \n",
    "                   metrics_graphics:bool = True,\n",
    "                   base_latlon:(float,float) = (-76.60532, 2.44091),\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"load addresses cache...\")\n",
    "    cache = Cache()\n",
    "    keys = sorted(list(cache.data.keys()))\n",
    "    lats = [cache.data[key][\"lat\"] for key in keys]\n",
    "    lons = [cache.data[key][\"lon\"] for key in keys]\n",
    "    print(f\"...loaded {len(keys)} addresses\")\n",
    "    print(f\"len(lons) = {len(lons)}\")\n",
    "    print(f\"len(lats) = {len(lats)}\")  \n",
    "    #\n",
    "    # load the traffic data...\n",
    "    #\n",
    "    print(\"load traffic data...\")\n",
    "    traffic = loader(verbose = True)\n",
    "    print(f\"...loaded {len(traffic)} records\")\n",
    "    #\n",
    "    # sort the data to find traffic...\n",
    "    #\n",
    "    slatlon = []\n",
    "    dlatlon = []\n",
    "    bps = []\n",
    "    dur = []\n",
    "    traffic_color = []\n",
    "    traffic_proto = []\n",
    "    num_records = len(traffic)\n",
    "    for i in tqdm(range(num_records)):\n",
    "        sip = traffic['Source.IP'].values[i]\n",
    "        dip = traffic['Destination.IP'].values[i]\n",
    "        keeper = False\n",
    "        if (public_address(sip)) and (not public_address(dip)):\n",
    "            # this is INBOUND traffic...\n",
    "            keeper = True\n",
    "            cache.search(sip)\n",
    "            traffic_color.append(\"green\")\n",
    "            slatlon.append((cache.data[sip][\"lon\"], cache.data[sip][\"lat\"]))\n",
    "            dlatlon.append(base_latlon)\n",
    "        elif (not public_address(sip)) and (public_address(dip)):\n",
    "            # this is OUTBOUND traffic...\n",
    "            keeper = True\n",
    "            cache.search(dip)\n",
    "            traffic_color.append(\"red\")\n",
    "            slatlon.append(base_latlon)\n",
    "            dlatlon.append((cache.data[dip][\"lon\"], cache.data[dip][\"lat\"]))\n",
    "        #\n",
    "        # now if this *is* a keeper let's append various things...\n",
    "        #\n",
    "        if (keeper):\n",
    "            bps.append(traffic['Flow.Bytes.s'].values[i])\n",
    "            dur.append(traffic['Flow.Duration'].values[i])\n",
    "            traffic_proto.append(traffic['ProtocolName'].values[i])\n",
    "    #\n",
    "    # status update...\n",
    "    #\n",
    "    print(f\"...recorded data for {len(slatlon)} transactions\")\n",
    "    #\n",
    "    # transparency of line will be based on traffic volume...\n",
    "    #\n",
    "    bps = [math.log10(b+10) for b in bps]\n",
    "    bps_max = np.max(bps)\n",
    "    bps = [b/bps_max for b in bps]\n",
    "    print(f\"normalized bps = ({np.min(bps)}, {np.median(bps)}, {np.max(bps)})\")\n",
    "    #\n",
    "    # width of line will be based on duration of transaction...\n",
    "    #\n",
    "    dur = [math.log10(d+10) for d in dur]\n",
    "    dur_max = np.max(dur)\n",
    "    dur = [d/dur_max for d in dur]\n",
    "    print(f\"normalized dur = ({np.min(dur)}, {np.median(dur)}, {np.max(dur)})\")\n",
    "    print(\"\")\n",
    "    #\n",
    "    # perhaps do pretty graphics...\n",
    "    #\n",
    "    print(\"...creating global traffic map\")\n",
    "    if (world_graphics):\n",
    "        display_world(slatlon, dlatlon, traffic_color, dur, bps, lons, lats, show_world = False)\n",
    "    #\n",
    "    # get the traffic protocol ranking...\n",
    "    #\n",
    "    print(\"...computing traffic protocol rankings\")\n",
    "    ranking = traffic_proto_ranking(traffic_proto)\n",
    "    #\n",
    "    # nearest integer function because (incredibly) there is none in numpy...\n",
    "    #\n",
    "    def nint(x:float):\n",
    "        return int(x + 0.5)\n",
    "    pct_total = 0.0\n",
    "    population = {}\n",
    "    for i in range(0, len(ranking)):\n",
    "        (percentage, proto) = ranking[i]\n",
    "        chance = nint(100.0 / percentage)\n",
    "        pct_total += percentage\n",
    "        print(\"pct_total: %9.5f%%, 1/likelihood: %6d, cumulative_pct: %9.5f%%, count: %6d, proto: '%s'\" %\n",
    "              (percentage, chance, pct_total, int((percentage/100)*len(dur)), proto))\n",
    "        population[proto] = {\n",
    "            \"rank\":i,\n",
    "            \"pct_total\":pct_total,\n",
    "            \"inv_likelihood\": chance,\n",
    "            \"cumulative_pct\":pct_total,\n",
    "            \"metrics\":{\n",
    "                \"path\":{\n",
    "                    \"src\":[],\n",
    "                    \"dst\":[],\n",
    "                },\n",
    "                \"flowrate\":{\n",
    "                    \"data\":[],\n",
    "                    \"count\":0,\n",
    "                    \"mode\":-1,\n",
    "                    \"median\":-1,\n",
    "                    \"mean\":-1,\n",
    "                    \"std\":-1,\n",
    "                    \"threshold\":-1,\n",
    "                },\n",
    "                \"duration\":{\n",
    "                    \"data\":[],\n",
    "                    \"count\":0,\n",
    "                    \"mode\":-1,\n",
    "                    \"median\":-1,\n",
    "                    \"mean\":-1,\n",
    "                    \"std\":-1,\n",
    "                    \"threshold\":-1,\n",
    "                },\n",
    "                \"volume\":{\n",
    "                    \"data\":[],\n",
    "                    \"count\":0,\n",
    "                    \"mode\":-1,\n",
    "                    \"median\":-1,\n",
    "                    \"mean\":-1,\n",
    "                    \"std\":-1,\n",
    "                    \"threshold\":-1,\n",
    "                },\n",
    "                \"avpktsz\":{ #\"Average.Packet.Size\"\n",
    "                    \"data\":[],\n",
    "                    \"count\":0,\n",
    "                    \"mode\":-1,\n",
    "                    \"median\":-1,\n",
    "                    \"mean\":-1,\n",
    "                    \"std\":-1,\n",
    "                    \"threshold\":-1,\n",
    "                },\n",
    "                \"dnuprat\":{#\"Down.Up.Ratio\"\n",
    "                    \"data\":[],\n",
    "                    \"count\":0,\n",
    "                    \"mode\":-1,\n",
    "                    \"median\":-1,\n",
    "                    \"mean\":-1,\n",
    "                    \"std\":-1,\n",
    "                    \"threshold\":-1,\n",
    "                },\n",
    "                \"pktlenvar\":{#\"Packet.Length.Variance\"\n",
    "                    \"data\":[],\n",
    "                    \"count\":0,\n",
    "                    \"mode\":-1,\n",
    "                    \"median\":-1,\n",
    "                    \"mean\":-1,\n",
    "                    \"std\":-1,\n",
    "                    \"threshold\":-1,\n",
    "                },\n",
    "                \"fwdpktlen\":{#\"Forward.Packet.Length.Mean\"\n",
    "                    \"data\":[],\n",
    "                    \"count\":0,\n",
    "                    \"mode\":-1,\n",
    "                    \"median\":-1,\n",
    "                    \"mean\":-1,\n",
    "                    \"std\":-1,\n",
    "                    \"threshold\":-1,\n",
    "                },\n",
    "                \"bwdpktlen\":{#\"Backward.Packet.Length.Mean\"\n",
    "                    \"data\":[],\n",
    "                    \"count\":0,\n",
    "                    \"mode\":-1,\n",
    "                    \"median\":-1,\n",
    "                    \"mean\":-1,\n",
    "                    \"std\":-1,\n",
    "                    \"threshold\":-1,\n",
    "                },\n",
    "                \"actmean\":{#\"Active.Mean\"\n",
    "                    \"data\":[],\n",
    "                    \"count\":0,\n",
    "                    \"mode\":-1,\n",
    "                    \"median\":-1,\n",
    "                    \"mean\":-1,\n",
    "                    \"std\":-1,\n",
    "                    \"threshold\":-1,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    #\n",
    "    # now compute population metrics by walking through the traffic data...\n",
    "    #\n",
    "    print(\"\\n...gathering population metrics data for flowrate and duration as function of protocol\")\n",
    "    for i in range(0, len(traffic)):\n",
    "        proto     = traffic['ProtocolName'].values[i]\n",
    "        flowrate  = traffic['Flow.Bytes.s'].values[i]\n",
    "        duration  = traffic['Flow.Duration'].values[i]\n",
    "        avpktsz   = traffic[\"Average.Packet.Size\"].values[i]\n",
    "        dnuprat   = traffic[\"Down.Up.Ratio\"].values[i]\n",
    "        pktlenvar = traffic[\"Packet.Length.Variance\"].values[i]\n",
    "        volume    = flowrate * duration\n",
    "        fwdpktlen = traffic[\"Fwd.Packet.Length.Mean\"].values[i]\n",
    "        bwdpktlen = traffic[\"Bwd.Packet.Length.Mean\"].values[i]\n",
    "        actmean   = traffic[\"Active.Mean\"].values[i]\n",
    "        sip       = traffic['Source.IP'].values[i]\n",
    "        dip       = traffic['Destination.IP'].values[i]\n",
    "        if ((public_address(sip) and not public_address(dip)) or\n",
    "            ((not public_address(sip)) and public_address(dip))):\n",
    "            #\n",
    "            # then it's an off-site transaction... so we can work with it...\n",
    "            #\n",
    "            population[proto][\"metrics\"][\"flowrate\"][\"data\"].append(flowrate)\n",
    "            population[proto][\"metrics\"][\"duration\"][\"data\"].append(duration)\n",
    "            population[proto][\"metrics\"][\"volume\"][\"data\"].append(volume)\n",
    "            population[proto][\"metrics\"][\"avpktsz\"][\"data\"].append(avpktsz)\n",
    "            population[proto][\"metrics\"][\"dnuprat\"][\"data\"].append(dnuprat)\n",
    "            population[proto][\"metrics\"][\"pktlenvar\"][\"data\"].append(pktlenvar)\n",
    "            population[proto][\"metrics\"][\"fwdpktlen\"][\"data\"].append(fwdpktlen)\n",
    "            population[proto][\"metrics\"][\"bwdpktlen\"][\"data\"].append(bwdpktlen)\n",
    "            population[proto][\"metrics\"][\"actmean\"][\"data\"].append(actmean)\n",
    "            if (public_address(sip) and (not public_address(dip))):\n",
    "                population[proto][\"metrics\"][\"path\"][\"src\"].append((cache.data[sip][\"lon\"], cache.data[sip][\"lat\"]))\n",
    "                population[proto][\"metrics\"][\"path\"][\"dst\"].append(base_latlon)\n",
    "            elif ((not public_address(sip)) and public_address(dip)):\n",
    "                population[proto][\"metrics\"][\"path\"][\"src\"].append(base_latlon)\n",
    "                population[proto][\"metrics\"][\"path\"][\"dst\"].append((cache.data[dip][\"lon\"], cache.data[dip][\"lat\"]))\n",
    "    #\n",
    "    # compute key statistics regarding the metrics...\n",
    "    #\n",
    "    print(\"...computing population metrics\")\n",
    "    for proto in population.keys():\n",
    "        #\n",
    "        # flowrate...\n",
    "        #\n",
    "        population[proto][\"metrics\"][\"flowrate\"][\"count\"]  = len(population[proto][\"metrics\"][\"flowrate\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"flowrate\"][\"threshold\"] = 1.0 / float(population[proto][\"metrics\"][\"flowrate\"][\"count\"])\n",
    "        population[proto][\"metrics\"][\"flowrate\"][\"mode\"]   = mode(population[proto][\"metrics\"][\"flowrate\"][\"data\"]).mode[0]\n",
    "        population[proto][\"metrics\"][\"flowrate\"][\"median\"] = np.median(population[proto][\"metrics\"][\"flowrate\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"flowrate\"][\"mean\"]   = np.mean(population[proto][\"metrics\"][\"flowrate\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"flowrate\"][\"std\"]    = np.std(population[proto][\"metrics\"][\"flowrate\"][\"data\"])\n",
    "        #\n",
    "        # duration...\n",
    "        #\n",
    "        population[proto][\"metrics\"][\"duration\"][\"count\"]  = len(population[proto][\"metrics\"][\"duration\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"duration\"][\"threshold\"] = 1.0 / float(population[proto][\"metrics\"][\"duration\"][\"count\"])\n",
    "        population[proto][\"metrics\"][\"duration\"][\"mode\"]   = mode(population[proto][\"metrics\"][\"duration\"][\"data\"]).mode[0]\n",
    "        population[proto][\"metrics\"][\"duration\"][\"median\"] = np.median(population[proto][\"metrics\"][\"duration\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"duration\"][\"mean\"]   = np.mean(population[proto][\"metrics\"][\"duration\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"duration\"][\"std\"]    = np.std(population[proto][\"metrics\"][\"duration\"][\"data\"])\n",
    "        #\n",
    "        # volume...\n",
    "        #\n",
    "        population[proto][\"metrics\"][\"volume\"][\"count\"]  = len(population[proto][\"metrics\"][\"volume\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"volume\"][\"threshold\"] = 1.0 / float(population[proto][\"metrics\"][\"volume\"][\"count\"])\n",
    "        population[proto][\"metrics\"][\"volume\"][\"mode\"]   = mode(population[proto][\"metrics\"][\"volume\"][\"data\"]).mode[0]\n",
    "        population[proto][\"metrics\"][\"volume\"][\"median\"] = np.median(population[proto][\"metrics\"][\"volume\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"volume\"][\"mean\"]   = np.mean(population[proto][\"metrics\"][\"volume\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"volume\"][\"std\"]    = np.std(population[proto][\"metrics\"][\"volume\"][\"data\"])\n",
    "        #\n",
    "        # avpktsz...\n",
    "        #\n",
    "        population[proto][\"metrics\"][\"avpktsz\"][\"count\"]  = len(population[proto][\"metrics\"][\"avpktsz\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"avpktsz\"][\"threshold\"] = 1.0 / float(population[proto][\"metrics\"][\"avpktsz\"][\"count\"])\n",
    "        population[proto][\"metrics\"][\"avpktsz\"][\"mode\"]   = mode(population[proto][\"metrics\"][\"avpktsz\"][\"data\"]).mode[0]\n",
    "        population[proto][\"metrics\"][\"avpktsz\"][\"median\"] = np.median(population[proto][\"metrics\"][\"avpktsz\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"avpktsz\"][\"mean\"]   = np.mean(population[proto][\"metrics\"][\"avpktsz\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"avpktsz\"][\"std\"]    = np.std(population[proto][\"metrics\"][\"avpktsz\"][\"data\"])\n",
    "        #\n",
    "        # dnuprat...\n",
    "        #\n",
    "        population[proto][\"metrics\"][\"dnuprat\"][\"count\"]  = len(population[proto][\"metrics\"][\"dnuprat\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"dnuprat\"][\"threshold\"] = 1.0 / float(population[proto][\"metrics\"][\"dnuprat\"][\"count\"])\n",
    "        population[proto][\"metrics\"][\"dnuprat\"][\"mode\"]   = mode(population[proto][\"metrics\"][\"dnuprat\"][\"data\"]).mode[0]\n",
    "        population[proto][\"metrics\"][\"dnuprat\"][\"median\"] = np.median(population[proto][\"metrics\"][\"dnuprat\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"dnuprat\"][\"mean\"]   = np.mean(population[proto][\"metrics\"][\"dnuprat\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"dnuprat\"][\"std\"]    = np.std(population[proto][\"metrics\"][\"dnuprat\"][\"data\"])\n",
    "        #\n",
    "        # fwdpktlen...\n",
    "        #\n",
    "        population[proto][\"metrics\"][\"fwdpktlen\"][\"count\"]  = len(population[proto][\"metrics\"][\"fwdpktlen\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"fwdpktlen\"][\"threshold\"] = 1.0 / float(population[proto][\"metrics\"][\"fwdpktlen\"][\"count\"])\n",
    "        population[proto][\"metrics\"][\"fwdpktlen\"][\"mode\"]   = mode(population[proto][\"metrics\"][\"fwdpktlen\"][\"data\"]).mode[0]\n",
    "        population[proto][\"metrics\"][\"fwdpktlen\"][\"median\"] = np.median(population[proto][\"metrics\"][\"fwdpktlen\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"fwdpktlen\"][\"mean\"]   = np.mean(population[proto][\"metrics\"][\"fwdpktlen\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"fwdpktlen\"][\"std\"]    = np.std(population[proto][\"metrics\"][\"fwdpktlen\"][\"data\"])\n",
    "\n",
    "        #\n",
    "        # avpktsz...\n",
    "        #\n",
    "        population[proto][\"metrics\"][\"bwdpktlen\"][\"count\"]  = len(population[proto][\"metrics\"][\"bwdpktlen\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"bwdpktlen\"][\"threshold\"] = 1.0 / float(population[proto][\"metrics\"][\"bwdpktlen\"][\"count\"])\n",
    "        population[proto][\"metrics\"][\"bwdpktlen\"][\"mode\"]   = mode(population[proto][\"metrics\"][\"bwdpktlen\"][\"data\"]).mode[0]\n",
    "        population[proto][\"metrics\"][\"bwdpktlen\"][\"median\"] = np.median(population[proto][\"metrics\"][\"bwdpktlen\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"bwdpktlen\"][\"mean\"]   = np.mean(population[proto][\"metrics\"][\"bwdpktlen\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"bwdpktlen\"][\"std\"]    = np.std(population[proto][\"metrics\"][\"bwdpktlen\"][\"data\"])\n",
    "        #\n",
    "        # dnuprat...\n",
    "        #\n",
    "        population[proto][\"metrics\"][\"actmean\"][\"count\"]  = len(population[proto][\"metrics\"][\"actmean\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"actmean\"][\"threshold\"] = 1.0 / float(population[proto][\"metrics\"][\"actmean\"][\"count\"])\n",
    "        population[proto][\"metrics\"][\"actmean\"][\"mode\"]   = mode(population[proto][\"metrics\"][\"actmean\"][\"data\"]).mode[0]\n",
    "        population[proto][\"metrics\"][\"actmean\"][\"median\"] = np.median(population[proto][\"metrics\"][\"actmean\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"actmean\"][\"mean\"]   = np.mean(population[proto][\"metrics\"][\"actmean\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"actmean\"][\"std\"]    = np.std(population[proto][\"metrics\"][\"actmean\"][\"data\"])\n",
    "        #\n",
    "        # pktlenvar...\n",
    "        #\n",
    "        population[proto][\"metrics\"][\"pktlenvar\"][\"count\"]  = len(population[proto][\"metrics\"][\"pktlenvar\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"pktlenvar\"][\"threshold\"] = 1.0 / float(population[proto][\"metrics\"][\"pktlenvar\"][\"count\"])\n",
    "        population[proto][\"metrics\"][\"pktlenvar\"][\"mode\"]   = mode(population[proto][\"metrics\"][\"pktlenvar\"][\"data\"]).mode[0]\n",
    "        population[proto][\"metrics\"][\"pktlenvar\"][\"median\"] = np.median(population[proto][\"metrics\"][\"pktlenvar\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"pktlenvar\"][\"mean\"]   = np.mean(population[proto][\"metrics\"][\"pktlenvar\"][\"data\"])\n",
    "        population[proto][\"metrics\"][\"pktlenvar\"][\"std\"]    = np.std(population[proto][\"metrics\"][\"pktlenvar\"][\"data\"])\n",
    "\n",
    "    #\n",
    "    # now perhaps show a table summarizing some of the metrics...\n",
    "    #\n",
    "    if (metrics_graphics):\n",
    "        display_metrics(population,\n",
    "                        min_count = 100, # probably 100 for production...\n",
    "                        num_bins = 10,  # still experimenting...\n",
    "                        show_histograms = False,\n",
    "                        show_table = True,\n",
    "        )\n",
    "\n",
    "    dill.dump(population,\n",
    "              gzip.open(os.path.join(places(\"datasets\"),\n",
    "                                     \"population_metrics.dill.gz\"), \"wb\"))\n",
    "    return None\n",
    "\n",
    "def scaled_rgba_v2(this_value:float, dpower:int = 0, min_value:float = 0.0, max_value:float = 1.0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if (dpower >= 5):\n",
    "        color = \"red\"\n",
    "        alpha = (this_value - min_value)/(max_value - min_value)\n",
    "    elif (dpower == 4):\n",
    "        color = \"orange\"\n",
    "        alpha = (this_value - min_value)/(max_value - min_value)\n",
    "    elif (dpower == 3):\n",
    "        color = \"yellow\"\n",
    "        alpha = (this_value - min_value)/(max_value - min_value)\n",
    "    elif (dpower == 2):\n",
    "        color = \"green\"\n",
    "        alpha = (this_value - min_value)/(max_value - min_value)\n",
    "    elif (dpower == 1):\n",
    "        color = \"blue\"\n",
    "        alpha = (this_value - min_value)/(max_value - min_value)\n",
    "    else:\n",
    "        color = \"indigo\"\n",
    "        alpha = (this_value - min_value)/(max_value - min_value)\n",
    "    return (color, alpha)\n",
    "\n",
    "def display_proto_radar(population:{},\n",
    "                        min_count:int = 1000,\n",
    "                        pseudo_sigmae:int = 5,\n",
    "                        anomaly_strategy:str = \"mean\", # \"mean\" or \"max\"...\n",
    "                        alpha_scaling:str = \"linear\",  # \"sqrt\", \"linear\", or \"square\"...\n",
    "                        dark_background:bool = False,\n",
    "                        black_facecolor:bool = False,\n",
    "                        display_graphics:bool = False):\n",
    "    \"\"\"\n",
    "    We do NOT have gaussian statistics here (things are definitely \n",
    "    NOT in a 'normal' distribution. So we're working with a sort of\n",
    "    pseudo_sigma in place of standard deviation.\n",
    "\n",
    "    We define the pseudo_sigma as follows:\n",
    "\n",
    "\n",
    "\n",
    "    @TODO: please improve this documentation\n",
    "\n",
    "    \"\"\"\n",
    "    ##\n",
    "    ## the labels for the radial coordinates...\n",
    "    ##\n",
    "    theta_labels = [#\"flowrate\",\n",
    "                    \"duration\", \"volume\",\n",
    "                    \"avpktsz\",  \"dnuprat\",  \"pktlenvar\",\n",
    "                    \"fwdpktlen\", \"bwdpktlen\",\n",
    "                   # \"actmean\",\n",
    "                   ]\n",
    "    ##\n",
    "    ## prepare the angular coordinates to be plotted...\n",
    "    ##\n",
    "    theta = [(2*pi*i/len(theta_labels)) for i in range(0, len(theta_labels))]\n",
    "    for proto in population.keys():\n",
    "        if (population[proto][\"metrics\"][\"flowrate\"][\"count\"] >= min_count):\n",
    "            radar_plot_name = os.path.join(places(\"images\"), f\"{proto}_radar.png\")\n",
    "            print(f\"...preparing radar plot for {proto}\")\n",
    "            ##\n",
    "            ## prepare the radial coordinates to be plotted...\n",
    "            ##\n",
    "            for theta_label in theta_labels:\n",
    "                num_values  = population[proto][\"metrics\"][theta_label][\"count\"]\n",
    "                data        = population[proto][\"metrics\"][theta_label][\"data\"]\n",
    "                data_min    = np.min(data)\n",
    "                data_max    = np.max(data)\n",
    "                data_mean   = population[proto][\"metrics\"][theta_label][\"mean\"]\n",
    "                data_median = population[proto][\"metrics\"][theta_label][\"median\"]\n",
    "                data_std    = population[proto][\"metrics\"][theta_label][\"std\"]\n",
    "                data_mode   = population[proto][\"metrics\"][theta_label][\"mode\"]\n",
    "                ##\n",
    "                ## our definition of anomaly (as defined in terms of pseudo_sigma)\n",
    "                ## may indicate that nothing IS an anomaly... that's fine...\n",
    "                ##\n",
    "                data_range = data_max - data_min\n",
    "                data_pos   = abs(data_max - data_mode)\n",
    "                data_neg   = abs(data_mode - data_min)\n",
    "                if (data_pos > 0) and (data_neg > 0):\n",
    "                    pseudo_sigma = np.mean([data_pos, data_neg]) / (pseudo_sigmae + 1)\n",
    "                elif (data_pos > 0):\n",
    "                    pseudo_sigma = data_pos / (pseudo_sigmae + 1)\n",
    "                elif (data_neg > 0):\n",
    "                    pseudo_sigma = data_neg / (pseudo_sigmae + 1)\n",
    "                #\n",
    "                # assigns an anomaly score on (0, pseudo_sigmae) to each of the various 'theta_labels' axes...\n",
    "                #\n",
    "                population[proto][\"metrics\"][theta_label][\"anomaly_score\"] = [abs(\n",
    "                        d-data_mode)/pseudo_sigma for d in data]\n",
    "                #\n",
    "                # if the radar plot figure doesn't already exist then create it...\n",
    "                #\n",
    "                if (not os.path.exists(radar_plot_name)):\n",
    "                    ##\n",
    "                    ## define the mean anomaly score for setting the color of the curves...\n",
    "                    ##\n",
    "                    anomaly_score = []\n",
    "                    for i in range(0, len(population[proto][\"metrics\"][theta_labels[0]][\"data\"])):\n",
    "                        if (anomaly_strategy == \"mean\"):\n",
    "                            anomaly_score.append(np.mean(\n",
    "                                [population[proto][\"metrics\"][theta_label][\"anomaly_score\"][i] for theta_label in theta_labels]))\n",
    "                        elif (anomaly_strategy == \"max\"):\n",
    "                            anomaly_score.append(np.max(\n",
    "                                [population[proto][\"metrics\"][theta_label][\"anomaly_score\"][i] for theta_label in theta_labels]))\n",
    "                    ##\n",
    "                    ## prepare the radial coordinates to be plotted...\n",
    "                    ##\n",
    "                    radius = []\n",
    "                    for i in range(0, len(population[proto][\"metrics\"][theta_labels[0]][\"data\"])):\n",
    "                        radius.append(\n",
    "                            [population[proto][\"metrics\"][theta_label][\"anomaly_score\"][i] for theta_label in theta_labels])\n",
    "                    ##\n",
    "                    ## create the figure...\n",
    "                    ##\n",
    "                    fig = plt.figure(figsize=(20,16))\n",
    "                    ax = fig.add_subplot(111, polar=True)\n",
    "                    ax.grid(True, lw=0.25)\n",
    "                    ##\n",
    "                    ## conditional appearance tweaks...\n",
    "                    ##\n",
    "                    if (dark_background):\n",
    "                        plt.style.use('dark_background')\n",
    "                    if (black_facecolor):\n",
    "                        plt.rcParams['axes.facecolor'] = 'black'\n",
    "                    plt.title(f\"Radar Plots of Feature Data for {len(anomaly_score)} {proto} Transactions\",\n",
    "                              fontsize=24, pad=24)\n",
    "                    plt.xlim((0, 2*pi))\n",
    "                    ##\n",
    "                    ## setting the radial scale...\n",
    "                    ##\n",
    "                    max_radius = math.ceil(np.max(radius))\n",
    "                    ax.set_rmax(max_radius) # new version using max_radius...\n",
    "                    ##\n",
    "                    ## choose the radial tick-marks to be drawn\n",
    "                    ##\n",
    "                    ax.set_rticks([r*0.5 for r in range(0, 2 * max_radius)]) # new version using max_radius...\n",
    "                    plt.xticks(theta, theta_labels, fontsize = 16)\n",
    "                    ans_min = np.min(anomaly_score)\n",
    "                    ans_max = np.max(anomaly_score)\n",
    "                    ans_mid = ans_max/2\n",
    "                    ans_med = np.median(anomaly_score)\n",
    "                    ans_mod = mode(anomaly_score).mode[0]\n",
    "                    ans_mea = np.mean(anomaly_score)\n",
    "                    sas = sorted([(anomaly_score[i], i) for i in range(0, len(anomaly_score))], reverse = True)\n",
    "                    #\n",
    "                    # a different approach...\n",
    "                    #\n",
    "                    d1_events = int(len(sas) / 10)\n",
    "                    if (d1_events > 0):\n",
    "                        d1_threshold = sas[d1_events][0]\n",
    "                        print(f\"number of d1 events: {d1_events}, d1_threshold = {d1_threshold}\")\n",
    "                    else:\n",
    "                        d1_threshold = 0\n",
    "                    d2_events = int(len(sas) / 100)\n",
    "                    if (d2_events > 0):\n",
    "                        d2_threshold = sas[d2_events][0]\n",
    "                        print(f\"number of d2 events: {d2_events}, d2_threshold = {d2_threshold}\")\n",
    "                    else:\n",
    "                        d2_threshold = 0\n",
    "                    d3_events = int(len(sas) / 1000)\n",
    "                    if (d3_events > 0):\n",
    "                        d3_threshold = sas[d3_events][0]\n",
    "                        print(f\"number of d3 events: {d3_events}, d3_threshold = {d3_threshold}\")\n",
    "                    else:\n",
    "                        d3_threshold = 0\n",
    "                    d4_events = int(len(sas) / 10000)\n",
    "                    if (d4_events > 0):\n",
    "                        d4_threshold = sas[d4_events][0]\n",
    "                        print(f\"number of d4 events: {d4_events}, d4_threshold = {d4_threshold}\")\n",
    "                    else:\n",
    "                        d4_threshold = 0\n",
    "                    d5_events = int(len(sas) / 100000)\n",
    "                    if (d5_events > 0):\n",
    "                        d5_threshold = sas[d5_events][0]\n",
    "                        print(f\"number of d5 events: {d5_events}, d5_threshold = {d5_threshold}\")\n",
    "                    else:\n",
    "                        d5_threshold = 0\n",
    "                    #\n",
    "                    # compute the colors...\n",
    "                    #\n",
    "                    graph_colors = []\n",
    "                    graph_labels = []\n",
    "                    legend_stuff = []\n",
    "                    for i in range(0, len(radius)):\n",
    "                        ##\n",
    "                        ## define the color for this particular curve...\n",
    "                        ##\n",
    "                        if (d5_events > 0) and (anomaly_score[i] > d5_threshold):\n",
    "                            # then it's a d5 event...\n",
    "                            (color, alpha) = scaled_rgba_v2(anomaly_score[i],\n",
    "                                                            dpower = 5,\n",
    "                                                            min_value = ans_min,\n",
    "                                                            max_value = ans_max,\n",
    "                            )\n",
    "                            if (color not in graph_colors):\n",
    "                                graph_colors.append(color)\n",
    "                                graph_labels.append(\"1 in 100000 event\")\n",
    "                        elif (d4_events > 0) and (anomaly_score[i] > d4_threshold):\n",
    "                            # then it's a d4 event...        \n",
    "                            (color, alpha) = scaled_rgba_v2(anomaly_score[i],\n",
    "                                                            dpower = 4,\n",
    "                                                            min_value = ans_min,\n",
    "                                                            max_value = ans_max,\n",
    "                            )\n",
    "                            if (color not in graph_colors):\n",
    "                                graph_colors.append(color)\n",
    "                                graph_labels.append(\"1 in 10000 event\")\n",
    "                        elif (d3_events > 0) and (anomaly_score[i] > d3_threshold):\n",
    "                            # then it's a d3 event...        \n",
    "                            (color, alpha) = scaled_rgba_v2(anomaly_score[i],\n",
    "                                                            dpower = 3,\n",
    "                                                            min_value = ans_min,\n",
    "                                                            max_value = ans_max,\n",
    "                            )\n",
    "                            if (color not in graph_colors):\n",
    "                                graph_colors.append(color)\n",
    "                                graph_labels.append(\"1 in 1000 event\")\n",
    "                        elif (d2_events > 0) and (anomaly_score[i] > d2_threshold):\n",
    "                            # then it's a d2 event...        \n",
    "                            (color, alpha) = scaled_rgba_v2(anomaly_score[i],\n",
    "                                                            dpower = 2,\n",
    "                                                            min_value = ans_min,\n",
    "                                                            max_value = ans_max,\n",
    "                            )\n",
    "                            if (color not in graph_colors):\n",
    "                                graph_colors.append(color)\n",
    "                                graph_labels.append(\"1 in 100 event\")\n",
    "                        elif (d1_events > 0) and (anomaly_score[i] > d1_threshold):\n",
    "                            # then it's a d1 event...        \n",
    "                            (color, alpha) = scaled_rgba_v2(anomaly_score[i],\n",
    "                                                            dpower = 1,\n",
    "                                                            min_value = ans_min,\n",
    "                                                            max_value = ans_max,\n",
    "                            )\n",
    "                            if (color not in graph_colors):\n",
    "                                graph_colors.append(color)\n",
    "                                graph_labels.append(\"1 in 10 event\")\n",
    "                        else:\n",
    "                            # then it's just a normal event...\n",
    "                            (color, alpha) = scaled_rgba_v2(anomaly_score[i],\n",
    "                                                            dpower = 0,\n",
    "                                                            min_value = ans_min,\n",
    "                                                            max_value = ans_max,\n",
    "                            )\n",
    "                            if (color not in graph_colors):\n",
    "                                graph_colors.append(color)\n",
    "                                graph_labels.append(\" common event\")\n",
    "\n",
    "                        this_theta = theta.copy()\n",
    "                        this_theta.append(theta[0])\n",
    "                        this_radius = radius[i].copy()\n",
    "                        this_radius.append(radius[i][0])\n",
    "                        if (alpha_scaling == \"sqrt\"):\n",
    "                            plt.plot(this_theta,\n",
    "                                     this_radius,\n",
    "                                     lw=2*(0.20 + sqrt(alpha)),\n",
    "                                     ls=\"-\",\n",
    "                                     color = color,\n",
    "                                     alpha = sqrt(alpha),\n",
    "                            )\n",
    "                        elif (alpha_scaling == \"square\"):\n",
    "                            plt.plot(this_theta,\n",
    "                                     this_radius,\n",
    "                                     lw=2*(0.20 + alpha**2),\n",
    "                                     ls=\"-\",\n",
    "                                     color = color,\n",
    "                                     alpha = alpha**2,\n",
    "                            )\n",
    "                        elif (alpha_scaling == \"linear\"):\n",
    "                            plt.plot(this_theta,\n",
    "                                     this_radius,\n",
    "                                     lw=2*(0.20 + alpha),\n",
    "                                     ls=\"-\",\n",
    "                                     color = color,\n",
    "                                     alpha = alpha,\n",
    "                            )\n",
    "                        else:\n",
    "                            raise RuntimeError(f\"Don't grok alpha_scaling == {alpha_scaling}\")\n",
    "                    #\n",
    "                    # create and render the legend...\n",
    "                    #\n",
    "                    legend_handles = []\n",
    "                    for i in range(0, len(graph_colors)):\n",
    "                        this_label = graph_labels[i]\n",
    "                        this_color = graph_colors[i]\n",
    "                        this_patch = mpatches.Patch(color = this_color, label = this_label)\n",
    "                        legend_handles.append(this_patch)\n",
    "                    ax.legend(loc = \"lower left\",\n",
    "                              handles = legend_handles,\n",
    "                              fontsize=14,\n",
    "                    )\n",
    "                    #\n",
    "                    # save things...\n",
    "                    #\n",
    "                    plt.savefig(radar_plot_name)\n",
    "                    if (display_graphics):\n",
    "                        plt.show()\n",
    "                    else:\n",
    "                        plt.close()\n",
    "                \n",
    "    updated_population_path = os.path.join(places(\"datasets\"), \"updated_population_metrics.dill.gz\")\n",
    "    dill.dump(population, gzip.open(updated_population_path, \"wb\"))\n",
    "    return None\n",
    "\n",
    "def histograms_and_radar_plots(anomaly_strategy:str = \"mean\",\n",
    "                               display_graphics:bool = False,\n",
    "                               alpha_scaling:str = \"square\",\n",
    "                               show_histograms:bool = False,\n",
    "                               pseudo_sigmae:int = 5,\n",
    "                               min_count:int = 100,\n",
    "                               main_title:int = 24,\n",
    "                               pane_title:int = 18,\n",
    "                               pane_text = 16,\n",
    "                               num_bins = 10,\n",
    "                               population_name:str = \"population_metrics.dill.gz\",\n",
    "                              ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"\\nrunning test_am...\")\n",
    "    anomaly_strategy = \"mean\"\n",
    "    display_graphics = False\n",
    "    alpha_scaling = \"square\"\n",
    "    show_histograms = False\n",
    "    pseudo_sigmae = 5\n",
    "    min_count = 100 # could be 30, 100, 300, 1000, 3000, etc\n",
    "    main_title = 24\n",
    "    pane_title = 18\n",
    "    pane_text = 16\n",
    "    num_bins = 10\n",
    "\n",
    "    ##\n",
    "    ## load the already-existing data...\n",
    "    ##\n",
    "    population_path = os.path.join(places(\"datasets\"), population_name)\n",
    "    if (not os.path.exists(population_path)):\n",
    "        raise RuntimeError(f\"Yikes!  Can't find {population_path}... have you run test_cachemap.py yet?\")\n",
    "    \n",
    "    population = dill.load(gzip.open(population_path, \"rb\"))\n",
    "    \n",
    "    colors = [\n",
    "        \"indigo\", \"darkblue\", \"blue\",\n",
    "        \"darkcyan\", \"green\", \"lightgreen\",\n",
    "        \"yellow\", \"orange\", \"red\",\n",
    "    ]\n",
    "\n",
    "    print(\"...histograms\")\n",
    "    for proto in population.keys():\n",
    "        #\n",
    "        # we're going to make 1 figure for each quantity for which we have at least 'min_count' observations\n",
    "        #\n",
    "        if (population[proto][\"metrics\"][\"flowrate\"][\"count\"] >= min_count):\n",
    "            #\n",
    "            # create the overall image and add the title...\n",
    "            #\n",
    "            fig, ax = plt.subplots(3, 3, figsize=(30, 30))\n",
    "            plt.suptitle(f\"Key Histograms for {proto}\", fontsize = main_title)\n",
    "            #\n",
    "            # 1st col, 1st row...\n",
    "            #\n",
    "            ax[0][0].set_title(\"Flow Rate (bytes/sec)\", fontsize = pane_title)\n",
    "            ax[0][0].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[0][0].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            ax[0][0].set_yscale('log')\n",
    "            ax[0][0].hist(population[proto][\"metrics\"][\"flowrate\"][\"data\"], color = colors[0],  bins = num_bins)\n",
    "            #\n",
    "            # 1st col, 2nd row...\n",
    "            #\n",
    "            ax[0][1].set_title(\"Duration (secs)\", fontsize = pane_title)\n",
    "            ax[0][1].set_yscale('log')\n",
    "            ax[0][1].hist(population[proto][\"metrics\"][\"duration\"][\"data\"], color = colors[1], bins = num_bins)\n",
    "            ax[0][1].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[0][1].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            #\n",
    "            # 1st col, 3rd row...\n",
    "            #\n",
    "            ax[0][2].set_title(\"Volume (bytes)\", fontsize = pane_title)\n",
    "            ax[0][2].set_yscale('log')\n",
    "            ax[0][2].hist(population[proto][\"metrics\"][\"volume\"][\"data\"], color = colors[2], bins = num_bins)\n",
    "            ax[0][2].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[0][2].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            #\n",
    "            # 2nd col, 1st row...\n",
    "            #\n",
    "            ax[1][0].set_title(\"Average Packet Size (bytes)\", fontsize = pane_title)\n",
    "            ax[1][0].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[1][0].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            ax[1][0].set_yscale('log')\n",
    "            ax[1][0].hist(population[proto][\"metrics\"][\"avpktsz\"][\"data\"], color = colors[3],  bins = num_bins)\n",
    "            #\n",
    "            # 2nd col, 2nd row...\n",
    "            #\n",
    "            ax[1][1].set_title(\"Down Up Ratio\", fontsize = pane_title)\n",
    "            ax[1][1].set_yscale('log')\n",
    "            ax[1][1].hist(population[proto][\"metrics\"][\"dnuprat\"][\"data\"], color = colors[4], bins = num_bins)\n",
    "            ax[1][1].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[1][1].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            #\n",
    "            # 2nd col, 3rd row...\n",
    "            #\n",
    "            ax[1][2].set_title(\"Packet Length Variance\", fontsize = pane_title)\n",
    "            ax[1][2].set_yscale('log')\n",
    "            ax[1][2].hist(population[proto][\"metrics\"][\"pktlenvar\"][\"data\"], color = colors[5], bins = num_bins)\n",
    "            ax[1][2].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[1][2].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            #\n",
    "            # 3rd col, 1st row...\n",
    "            #\n",
    "            ax[2][0].set_title(\"Forward Packet Length Mean (bytes)\", fontsize = pane_title)\n",
    "            ax[2][0].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[2][0].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            ax[2][0].set_yscale('log')\n",
    "            ax[2][0].hist(population[proto][\"metrics\"][\"fwdpktlen\"][\"data\"], color = colors[6],  bins = num_bins)\n",
    "            #\n",
    "            # 3rd col, 2nd row...\n",
    "            #\n",
    "            ax[2][1].set_title(\"Backward Packet Length Mean (bytes)\", fontsize = pane_title)\n",
    "            ax[2][1].set_yscale('log')\n",
    "            ax[2][1].hist(population[proto][\"metrics\"][\"bwdpktlen\"][\"data\"], color = colors[7], bins = num_bins)\n",
    "            ax[2][1].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[2][1].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            #\n",
    "            # 3rd col, 3rd row...\n",
    "            #\n",
    "            ax[2][2].set_title(\"Active Mean\", fontsize = pane_title)\n",
    "            ax[2][2].set_yscale('log')\n",
    "            ax[2][2].hist(population[proto][\"metrics\"][\"actmean\"][\"data\"], color = colors[8], bins = num_bins)\n",
    "            ax[2][2].set_xlabel(\"Bin\", fontsize = pane_text)\n",
    "            ax[2][2].set_ylabel(\"log10(Counts)\", fontsize = pane_text)\n",
    "            #\n",
    "            # save/display or whatever...\n",
    "            #\n",
    "            plt.savefig(os.path.join(places(\"images\"), f\"{proto}_histogram.png\"))\n",
    "            if (show_histograms):\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "\n",
    "    print(\"...radar plots\")\n",
    "    display_proto_radar(population,\n",
    "                        display_graphics = display_graphics,\n",
    "                        min_count = min_count,\n",
    "                        pseudo_sigmae = pseudo_sigmae,\n",
    "                        anomaly_strategy = anomaly_strategy,\n",
    "                        alpha_scaling = alpha_scaling)\n",
    "    print(\"...done\")\n",
    "    return None\n",
    "\n",
    "def create_traffic_maps_by_protocol(population_name:str = \"updated_population_metrics.dill.gz\",\n",
    "                                    min_count:int = 1000, \n",
    "                                    anomaly_scaling:str = \"linear\", \n",
    "                                    show_world:bool = False,\n",
    "                                    main_title:int = 24,\n",
    "                                    pane_title:int = 18,\n",
    "                                    pane_text:int = 16,\n",
    "                                    uni_latlon:(float,float) = (-76.60532, 2.44091),\n",
    "                                    labels:[str] = [\"duration\", \"volume\", \"avpktsz\",  \"dnuprat\",  \n",
    "                                                    \"pktlenvar\", \"fwdpktlen\", \"bwdpktlen\"]):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    population = dill.load(gzip.open(os.path.join(places(\"datasets\"), population_name), \"rb\"))    \n",
    "    for proto in population.keys():\n",
    "        locations = population[proto][\"metrics\"][\"path\"]\n",
    "        slatlon = locations[\"src\"]\n",
    "        dlatlon = locations[\"dst\"]\n",
    "        npts = len(population[proto]['metrics']['flowrate']['data'])\n",
    "        if (npts >= min_count):\n",
    "            ascore = []\n",
    "            tcolor = []\n",
    "            for i in range(0, npts):\n",
    "                asmean = np.mean([population[proto]['metrics'][label]['anomaly_score'][i] for label in labels])\n",
    "                ascore.append(asmean)\n",
    "            as_max = np.max(ascore)\n",
    "            as_mid = as_max / 2\n",
    "            as_mea = np.mean(ascore)\n",
    "            as_med = np.median(ascore)\n",
    "            as_mod = mode(ascore).mode[0]\n",
    "            as_min = np.min(ascore)\n",
    "            anomaly_score = ascore.copy()\n",
    "            print(f\"...ascore for {proto} contains {len(np.unique(ascore))} values with:\")\n",
    "            sas = sorted([(anomaly_score[i], i) for i in range(0, len(anomaly_score))], reverse = True)\n",
    "            #\n",
    "            # a different approach...\n",
    "            #\n",
    "            d1_events = int(len(sas) / 10)\n",
    "            if (d1_events > 0):\n",
    "                d1_threshold = sas[d1_events][0]\n",
    "                print(f\"number of d1 events: {d1_events}, d1_threshold = {d1_threshold}\")\n",
    "            else:\n",
    "                d1_threshold = 0\n",
    "            d2_events = int(len(sas) / 100)\n",
    "            if (d2_events > 0):\n",
    "                d2_threshold = sas[d2_events][0]\n",
    "                print(f\"number of d2 events: {d2_events}, d2_threshold = {d2_threshold}\")\n",
    "            else:\n",
    "                d2_threshold = 0\n",
    "            d3_events = int(len(sas) / 1000)\n",
    "            if (d3_events > 0):\n",
    "                d3_threshold = sas[d3_events][0]\n",
    "                print(f\"number of d3 events: {d3_events}, d3_threshold = {d3_threshold}\")\n",
    "            else:\n",
    "                d3_threshold = 0\n",
    "            d4_events = int(len(sas) / 10000)\n",
    "            if (d4_events > 0):\n",
    "                d4_threshold = sas[d4_events][0]\n",
    "                print(f\"number of d4 events: {d4_events}, d4_threshold = {d4_threshold}\")\n",
    "            else:\n",
    "                d4_threshold = 0\n",
    "            d5_events = int(len(sas) / 100000)\n",
    "            if (d5_events > 0):\n",
    "                d5_threshold = sas[d5_events][0]\n",
    "                print(f\"number of d5 events: {d5_events}, d5_threshold = {d5_threshold}\")\n",
    "            else:\n",
    "                d5_threshold = 0\n",
    "            #\n",
    "            # compute the colors...\n",
    "            #\n",
    "            print(f\"...preparing colors and labels\")\n",
    "            graph_colors = []\n",
    "            graph_labels = []\n",
    "            vector_color = []\n",
    "            vector_alpha = []\n",
    "            for i in range(0, npts):\n",
    "                if (d5_events > 0) and (anomaly_score[i] > d5_threshold):\n",
    "                    # then it's a d5 event...\n",
    "                    (color, alpha) = scaled_rgba_v2(anomaly_score[i], dpower = 5, min_value = as_min, max_value = as_max)\n",
    "                    vector_color.append(color)\n",
    "                    vector_alpha.append(alpha)\n",
    "                    if (color not in graph_colors):\n",
    "                        graph_colors.append(color)\n",
    "                        graph_labels.append(\"1 in 100000 event\")\n",
    "                elif (d4_events > 0) and (anomaly_score[i] > d4_threshold):\n",
    "                    # then it's a d4 event...        \n",
    "                    (color, alpha) = scaled_rgba_v2(anomaly_score[i], dpower = 4, min_value = as_min, max_value = as_max)\n",
    "                    vector_color.append(color)\n",
    "                    vector_alpha.append(alpha)\n",
    "                    if (color not in graph_colors):\n",
    "                        graph_colors.append(color)\n",
    "                        graph_labels.append(\"1 in 10000 event\")\n",
    "                elif (d3_events > 0) and (anomaly_score[i] > d3_threshold):\n",
    "                    # then it's a d3 event...        \n",
    "                    (color, alpha) = scaled_rgba_v2(anomaly_score[i], dpower = 3, min_value = as_min, max_value = as_max)\n",
    "                    vector_color.append(color)\n",
    "                    vector_alpha.append(alpha)\n",
    "                    if (color not in graph_colors):\n",
    "                        graph_colors.append(color)\n",
    "                        graph_labels.append(\"1 in 1000 event\")\n",
    "                elif (d2_events > 0) and (anomaly_score[i] > d2_threshold):\n",
    "                    # then it's a d2 event...        \n",
    "                    (color, alpha) = scaled_rgba_v2(anomaly_score[i], dpower = 2, min_value = as_min, max_value = as_max)\n",
    "                    vector_color.append(color)\n",
    "                    vector_alpha.append(alpha)\n",
    "                    if (color not in graph_colors):\n",
    "                        graph_colors.append(color)\n",
    "                        graph_labels.append(\"1 in 100 event\")\n",
    "                elif (d1_events > 0) and (anomaly_score[i] > d1_threshold):\n",
    "                    # then it's a d1 event...        \n",
    "                    (color, alpha) = scaled_rgba_v2(anomaly_score[i], dpower = 1, min_value = as_min, max_value = as_max)\n",
    "                    vector_color.append(color)\n",
    "                    vector_alpha.append(alpha)\n",
    "                    if (color not in graph_colors):\n",
    "                        graph_colors.append(color)\n",
    "                        graph_labels.append(\"1 in 10 event\")\n",
    "                else:\n",
    "                    # then it's just a normal event...\n",
    "                    (color, alpha) = scaled_rgba_v2(anomaly_score[i], dpower = 0, min_value = as_min, max_value = as_max)\n",
    "                    vector_color.append(color)\n",
    "                    vector_alpha.append(alpha)\n",
    "                    if (color not in graph_colors):\n",
    "                        graph_colors.append(color)\n",
    "                        graph_labels.append(\" common event\")\n",
    "\n",
    "            print(f\"...preparing {proto} end points\")\n",
    "            lons = []\n",
    "            lats = []\n",
    "            dire = []\n",
    "            for i in range(0, npts):\n",
    "                if (slatlon[i] == uni_latlon):\n",
    "                    lons.append(slatlon[i][0])\n",
    "                    lats.append(slatlon[i][1])\n",
    "                    dire.append(\"outbound\")\n",
    "            for i in range(0, npts):\n",
    "                if (dlatlon[i] == uni_latlon):\n",
    "                    lons.append(dlatlon[i][0])\n",
    "                    lats.append(dlatlon[i][1])\n",
    "                    dire.append(\"inbound\")\n",
    "            #\n",
    "            # okay show time!\n",
    "            #\n",
    "            fig, ax = plt.subplots(2, 1, figsize = (32, 36))\n",
    "            worldmap = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "            plt.suptitle(f\"Traffic Vectors for {proto}\")\n",
    "            #========================================================================\n",
    "            #\n",
    "            # the inbound traffic map...\n",
    "            #\n",
    "            #========================================================================\n",
    "            worldmap.plot(color = \"lightgrey\", ax = ax[0])\n",
    "            ax[0].set_xlim((-180.0, 180.0))\n",
    "            ax[0].set_xlabel(\"Longitude\", fontsize = pane_text)\n",
    "            ax[0].set_ylim((-90.0, 90.0))\n",
    "            ax[0].set_ylabel(\"Latitude\", fontsize = pane_text)\n",
    "            print(f\"...inbound {proto} vectors\")\n",
    "            count = 0\n",
    "            for i in range(0, npts):\n",
    "                if (dire[i] == \"inbound\"):\n",
    "                    count += 1\n",
    "                    this_color = vector_color[i]\n",
    "                    this_alpha = vector_alpha[i]\n",
    "                    if (anomaly_scaling == \"square\"):\n",
    "                        this_alpha = vector_alpha[i]**2\n",
    "                    elif (anomaly_scaling == \"sqrt\"):\n",
    "                        this_alpha = sqrt(vector_alpha[i])\n",
    "                    elif (anomaly_scaling == \"linear\"):\n",
    "                        this_alpha = vector_alpha[i]\n",
    "                    ax[0].plot([slatlon[i][0], dlatlon[i][0]],\n",
    "                               [slatlon[i][1], dlatlon[i][1]], \n",
    "                               color = this_color,\n",
    "                               alpha = min((0.10 + this_alpha), 1), # this_alpha\n",
    "                               lw = 4*(0.25 + 1.75 * this_alpha), # (0.25 + 5 * this_alpha),\n",
    "                    )\n",
    "            ax[0].set_title(f\"{count} Inbound {proto} Vectors\", fontsize = pane_title)\n",
    "            #\n",
    "            # create and render the legend...\n",
    "            #\n",
    "            legend_handles = []\n",
    "            for i in range(0, len(graph_colors)):\n",
    "                this_label = graph_labels[i]\n",
    "                this_color = graph_colors[i]\n",
    "                this_patch = mpatches.Patch(color = this_color, label = this_label)\n",
    "                legend_handles.append(this_patch)\n",
    "            ax[0].legend(loc = \"lower left\", handles = legend_handles)\n",
    "            #========================================================================\n",
    "            #\n",
    "            # the outbound traffic map...\n",
    "            #\n",
    "            #========================================================================\n",
    "            worldmap.plot(color = \"lightgrey\", ax = ax[1])\n",
    "            ax[1].set_xlim((-180.0, 180.0))\n",
    "            ax[1].set_xlabel(\"Longitude\", fontsize = pane_text)\n",
    "            ax[1].set_ylim((-90.0, 90.0))\n",
    "            ax[1].set_ylabel(\"Latitude\", fontsize = pane_text)\n",
    "            print(f\"...outbound {proto} vectors\")\n",
    "            count = 0\n",
    "            for i in range(0, npts):\n",
    "                if (dire[i] == \"outbound\"):\n",
    "                    count += 1\n",
    "                    this_color = vector_color[i]\n",
    "                    this_alpha = vector_alpha[i]\n",
    "                    if (anomaly_scaling == \"square\"):\n",
    "                        this_alpha = vector_alpha[i]**2\n",
    "                    elif (anomaly_scaling == \"sqrt\"):\n",
    "                        this_alpha = sqrt(vector_alpha[i])\n",
    "                    elif (anomaly_scaling == \"linear\"):\n",
    "                        this_alpha = vector_alpha[i]\n",
    "                    ax[1].plot([slatlon[i][0], dlatlon[i][0]],\n",
    "                               [slatlon[i][1], dlatlon[i][1]], \n",
    "                               color = this_color,\n",
    "                               alpha = min((0.10 + this_alpha), 1), # this_alpha\n",
    "                               lw = 4*(0.25 + 1.75 * this_alpha), # (0.25 + 5 * this_alpha),\n",
    "                    )\n",
    "            ax[1].set_title(f\"{count} Outbound {proto} Vectors\", fontsize = pane_title)\n",
    "            #\n",
    "            # create and render the legend...\n",
    "            #\n",
    "            legend_handles = []\n",
    "            for i in range(0, len(graph_colors)):\n",
    "                this_label = graph_labels[i]\n",
    "                this_color = graph_colors[i]\n",
    "                this_patch = mpatches.Patch(color = this_color, label = this_label)\n",
    "                legend_handles.append(this_patch)\n",
    "            ax[1].legend(loc = \"lower left\", handles = legend_handles)\n",
    "            #========================================================================\n",
    "            #\n",
    "            # save everything...\n",
    "            #\n",
    "            #========================================================================\n",
    "            plt.savefig(os.path.join(places(\"images\"), f\"{proto}_traffic.png\"))\n",
    "            if (show_world):\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "    return None\n",
    "\n",
    "def download_from_google_drive(shared_with_anyone_url:str, output_path:str) -> bool:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cmdline = f\"`which python | sed s/'\\/python'/''/`/gdown --fuzzy {shared_with_anyone_url} --output {output_path}\"\n",
    "    print(f\"Attempting to run '{cmdline}'\")\n",
    "    status = os.system(cmdline)\n",
    "    if (status == 0):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fdd88be5-6f63-472a-a109-b3fda0c56365",
     "showTitle": true,
     "title": "Possibly Fetch Data"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...found /Volumes/blueprint/ncemc/battery/datasets/Dataset-Unicauca-Version2-87Atts.parquet.gzip already present.\n"
     ]
    }
   ],
   "source": [
    "## Fetch datafile if don't have locally...\n",
    "dst = os.path.join(places(\"datasets\"), \"Dataset-Unicauca-Version2-87Atts.parquet.gzip\")\n",
    "if (not os.path.exists(dst)):\n",
    "    src = \"https://drive.google.com/file/d/1Z9M48CwJbbRyLEYUv-qwSx1kFJZPyF84/view?usp=sharing\"\n",
    "    download_from_google_drive(src, dst)\n",
    "else:\n",
    "    print(f\"...found {dst} already present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e33db67-f595-4fa0-b0bd-ba6d585fe9f8",
     "showTitle": true,
     "title": "Datasets"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/blueprint/ncemc/battery/datasets contains:\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/wx_stations_with_location.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/cloud_nodetypes.xls\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/cloud_nodetypes.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1727715107.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/tri_intel_monitor.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/the-verdict.txt\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/functional_test_dummy_data_metrics_history.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/old_ts300.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/covtype_train.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/antipattern_4_example_1.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/clean_flt_ts3.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_demo_holdout.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/smoothing_test_2.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts1.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_demo_training_2b.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/crime_desc_code.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts300g.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_demo_testing.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/tri_nvidia_monitor.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1727714175.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1727790957.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/d_demo1_sample_datadict1.dill.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/clean_flt_ts3.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/pnode_locations_raw.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/no_antipattern_example.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_demo_testing.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/None.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/filtered_5pct_wx_stations_with_location.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts300g.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1727384435.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/antipattern_2_example.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/46_gutenberg_books\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/cal_housing_py3.pkz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1727713324.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/uuid_set_small.tsv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_1000000rows_101cols.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1731683182.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/test10.fits\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ceir_nvidia_monitor.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts2.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/Dataset-Unicauca-Version2-87Atts.parquet.snappy\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1727714728.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/tradeable_nodes.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts300_2.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts3.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/Dataset-Unicauca-Version2-87Atts.parquet.lz4\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_demo_training.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1727725563.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/wx_stations_subset.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1731682522.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/uscities_modified_v3.tsv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/simple_count.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/annual_05_051_0003_2023.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/Dataset-Unicauca-Version2-87Atts.parquet.gzip\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/c_demo1_sample_datadict1.dill.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/aon_nvidia_monitor.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/databricks_nodeinfo.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts3_HOURLY.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/old_ts300.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/dsgtest3_datadict.dill.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts3p.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts300_2a.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts300.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts300_2a.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/antipattern_3_example_2.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/antipattern_3_example_1.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts300_2b.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_demo_sm.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1731682496.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/b_demo1_sample_datadict1.dill.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_demo_holdout.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/test1000.fits\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1727386938.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_10000rows_101cols.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_demo.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1727702923.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/smoothing_test_1.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_100000rows_101cols.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/demo1_sample_datadict1.dill.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts300.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_demo_training_2a.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/covtype_test.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts300.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/Dataset-Unicauca-Version2-87Atts.parquet.zstd\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/replicate.py\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts300_2.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/voyager_data.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/dha_nvidia_monitor.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts1.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/pn2wx.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/natural_earth\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/dha_intel_monitor.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/Dataset-Unicauca-Version2-87Atts.parquet.brotli\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/intel_cluster__1709905369.6747820.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ceir_intel_monitor.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts300u.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/alien_nvidia_monitor.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/Dataset-Unicauca-Version2-87Atts.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts3.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/resource_utilization_data.dill\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/Dataset-Unicauca-Version2-87Atts.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/alien_intel_monitor.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_30000rows_101cols.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_300000rows_101cols.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/tradeable_locations.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/samsara_data.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/100329_gutenberg_books\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1727713737.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_demo.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/viewpoint_data.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/uscities_modified_v3.xlsx\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts3_DAILY.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/1893_gutenberg_books\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts300_2b.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts2.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/regression_demo_training.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/tfds\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/gutenberg\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/codon_test.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1727791258.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/daily_05_051_0003_2023.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/The BPCS Shuffler(1-39).xlsx\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1727790449.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/ts3.csv.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/test1-1.fits\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/nvidia_cluster__1709905371.0269578.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/watcher_1731682379.parquet\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/a_demo1_sample_datadict1.dill.gz\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/antipattern_2_example_1.csv\n",
      "   /Volumes/blueprint/ncemc/battery/datasets/test.csv\n"
     ]
    }
   ],
   "source": [
    "## Tiny code fragment showing what's in the places(\"datasets\") directory...\n",
    "data_dir = places(\"datasets\")\n",
    "print(f\"{data_dir} contains:\")\n",
    "for file_name in os.listdir(data_dir):\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    print(f\"   {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "59b9c339-a3df-476d-8b81-d9531598e616",
     "showTitle": true,
     "title": "Populate Cache and Create Global Traffic Map"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load addresses cache...\n",
      "...loaded 0 addresses\n",
      "len(lons) = 0\n",
      "len(lats) = 0\n",
      "load traffic data...\n",
      "found /Volumes/blueprint/ncemc/battery/datasets/Dataset-Unicauca-Version2-87Atts.parquet.gzip, loading...\n",
      "read dataframe with shape (3577296, 87) in 4.499 secs\n",
      "...loaded 3577296 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1129/3577296 [05:00<322:55:35,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3130/3577296 [10:00<93:10:57, 10.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5571/3577296 [15:00<163:48:49,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8719/3577296 [20:01<80:28:35, 12.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11153/3577296 [25:01<191:38:50,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13367/3577296 [30:12<143:29:46,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17541/3577296 [35:12<76:44:13, 12.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21715/3577296 [40:13<37:31:28, 26.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 23861/3577296 [45:15<459:46:43,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26370/3577296 [50:16<78:47:05, 12.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 30421/3577296 [55:16<37:50:24, 26.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 33594/3577296 [1:00:16<91:32:04, 10.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 35567/3577296 [1:05:17<195:38:06,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 38493/3577296 [1:10:17<111:57:42,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 42931/3577296 [1:15:18<94:43:13, 10.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 46708/3577296 [1:20:18<83:43:36, 11.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|         | 50548/3577296 [1:25:19<94:04:32, 10.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 55156/3577296 [1:30:19<60:59:09, 16.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 59330/3577296 [1:35:20<59:41:17, 16.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 64691/3577296 [1:40:20<56:21:36, 17.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 70518/3577296 [1:45:21<44:41:18, 21.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 97218/3577296 [1:50:22<10:15:08, 94.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 130616/3577296 [1:55:13<339:50:01,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 132429/3577296 [2:00:23<161:29:59,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 133685/3577296 [2:05:23<326:40:41,  2.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 136810/3577296 [2:10:24<222:31:28,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 139751/3577296 [2:15:21<255:00:34,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 140871/3577296 [2:20:22<483:56:30,  1.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 143026/3577296 [2:25:25<79:00:39, 12.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 147738/3577296 [2:30:26<59:58:12, 15.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 152519/3577296 [2:35:27<44:52:46, 21.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 153729/3577296 [2:40:24<503:05:43,  1.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 156537/3577296 [2:45:28<44:51:30, 21.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 159577/3577296 [2:50:25<44:07:47, 21.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 162672/3577296 [2:55:31<87:11:23, 10.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 166349/3577296 [3:00:31<109:04:03,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 170566/3577296 [3:05:29<226:29:12,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 172188/3577296 [3:10:33<127:55:33,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 174411/3577296 [3:15:30<98:35:22,  9.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 176706/3577296 [3:20:33<60:30:45, 15.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 182780/3577296 [3:25:33<28:05:51, 33.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 187273/3577296 [3:30:34<370:32:43,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 189635/3577296 [3:35:35<330:29:41,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 191862/3577296 [3:40:35<121:47:41,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 194316/3577296 [3:45:36<64:14:13, 14.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 197928/3577296 [3:50:36<144:05:47,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 304265/3577296 [3:55:40<1:34:18, 578.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 308456/3577296 [4:00:38<121:14:49,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 310186/3577296 [4:05:39<170:05:24,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 312472/3577296 [4:10:40<108:24:31,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 100 new addresses, updating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 314169/3577296 [4:14:07<319:15:58,  2.84it/s]"
     ]
    }
   ],
   "source": [
    "populate_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f54e6a8-a53d-4883-8abb-739ba6565c2a",
     "showTitle": true,
     "title": "Create Histograms and Radar Plots by Protocol"
    }
   },
   "outputs": [],
   "source": [
    "histograms_and_radar_plots(min_count = 1000, alpha_scaling = \"square\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8be0edc7-25fe-4ac2-87e3-dea71071cd35",
     "showTitle": true,
     "title": "Create Anomaly Traffic Maps by Protocol"
    }
   },
   "outputs": [],
   "source": [
    "create_traffic_maps_by_protocol(min_count = 1000, anomaly_scaling = \"square\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "387a9961-e476-44f7-9a0f-834435973a93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Primus",
   "notebookOrigID": 3662742996701583,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
